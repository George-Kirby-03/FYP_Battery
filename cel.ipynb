{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0535b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))  \n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5d5f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.float32 torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=32)\n",
    "for X, y in train_iter:\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0752461e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "net.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8aa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.218377, accuracy 30.000000\n",
      "epoch 2, loss 0.389133, accuracy 30.000000\n",
      "epoch 3, loss 0.420529, accuracy 29.000000\n",
      "epoch 4, loss 0.338723, accuracy 28.000000\n",
      "epoch 5, loss 0.349119, accuracy 29.000000\n",
      "epoch 6, loss 0.715697, accuracy 26.000000\n",
      "epoch 7, loss 0.126484, accuracy 32.000000\n",
      "epoch 8, loss 0.449280, accuracy 27.000000\n",
      "epoch 9, loss 0.371843, accuracy 30.000000\n",
      "epoch 10, loss 0.106675, accuracy 32.000000\n",
      "epoch 11, loss 0.454163, accuracy 27.000000\n",
      "epoch 12, loss 0.516590, accuracy 28.000000\n",
      "epoch 13, loss 0.582746, accuracy 24.000000\n",
      "epoch 14, loss 0.406100, accuracy 30.000000\n",
      "epoch 15, loss 0.445568, accuracy 27.000000\n",
      "epoch 16, loss 0.294844, accuracy 30.000000\n",
      "epoch 17, loss 0.344805, accuracy 29.000000\n",
      "epoch 18, loss 0.344926, accuracy 30.000000\n",
      "epoch 19, loss 0.433578, accuracy 29.000000\n",
      "epoch 20, loss 0.300857, accuracy 31.000000\n",
      "epoch 21, loss 0.424733, accuracy 29.000000\n",
      "epoch 22, loss 0.297320, accuracy 29.000000\n",
      "epoch 23, loss 0.504771, accuracy 29.000000\n",
      "epoch 24, loss 0.323247, accuracy 30.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m num_epochs = \u001b[32m150\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/datasets/mnist.py:143\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    139\u001b[39m img, target = \u001b[38;5;28mself\u001b[39m.data[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.targets[index])\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m img = \u001b[43m_Image_fromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    146\u001b[39m     img = \u001b[38;5;28mself\u001b[39m.transform(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/utils.py:251\u001b[39m, in \u001b[36m_Image_fromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m    248\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    249\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to convert obj into contiguous format\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontiguous_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Image.fromarray(obj, mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/PIL/Image.py:3198\u001b[39m, in \u001b[36mfrombuffer\u001b[39m\u001b[34m(mode, size, data, decoder_name, *args)\u001b[39m\n\u001b[32m   3196\u001b[39m     args = mode, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m\n\u001b[32m   3197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m _MAPMODES:\n\u001b[32m-> \u001b[39m\u001b[32m3198\u001b[39m     im = \u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3199\u001b[39m     im = im._new(core.map_buffer(data, size, decoder_name, \u001b[32m0\u001b[39m, args))\n\u001b[32m   3200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/PIL/Image.py:3099\u001b[39m, in \u001b[36mnew\u001b[39m\u001b[34m(mode, size, color)\u001b[39m\n\u001b[32m   3097\u001b[39m         im.palette = ImagePalette.ImagePalette()\n\u001b[32m   3098\u001b[39m         color = im.palette.getcolor(color_ints)\n\u001b[32m-> \u001b[39m\u001b[32m3099\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m im._new(\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_iter:\n",
    "        l = loss_fn(net(X), y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    acc = accuracy(net(X), y)\n",
    "    print(f'epoch {epoch + 1},accuracy {acc:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba423466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss 0.6217 | Train Acc 0.7913 | Test Acc 0.7941\n",
      "Epoch  2 | Loss 0.4882 | Train Acc 0.8324 | Test Acc 0.8251\n",
      "Epoch  3 | Loss 0.4649 | Train Acc 0.8404 | Test Acc 0.8316\n",
      "Epoch  4 | Loss 0.4502 | Train Acc 0.8438 | Test Acc 0.8306\n",
      "Epoch  5 | Loss 0.4398 | Train Acc 0.8499 | Test Acc 0.8337\n",
      "Epoch  6 | Loss 0.4341 | Train Acc 0.8498 | Test Acc 0.8401\n",
      "Epoch  7 | Loss 0.4273 | Train Acc 0.8523 | Test Acc 0.8392\n",
      "Epoch  8 | Loss 0.4250 | Train Acc 0.8531 | Test Acc 0.8411\n",
      "Epoch  9 | Loss 0.4230 | Train Acc 0.8538 | Test Acc 0.8418\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     68\u001b[39m     train_loss_sum, train_acc_sum, n = \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = _Image_fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/transforms/functional.py:168\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    167\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m img = torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Load dataset\n",
    "# ------------------------------\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    \n",
    "    mnist_train = datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)\n",
    "    \n",
    "    return (data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True),\n",
    "            data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False))\n",
    "\n",
    "batch_size = 60\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Define linear model (no ReLU)\n",
    "# ------------------------------\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),         # flatten 28x28 → 784\n",
    "    nn.Linear(784, 10)\n",
    ")\n",
    "\n",
    "# Initialize weights\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Define loss and optimizer\n",
    "# ------------------------------\n",
    "loss_fn = nn.CrossEntropyLoss()         # includes Softmax internally\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ Accuracy function\n",
    "# ------------------------------\n",
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    return (y_hat == y).float().sum().item()\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    acc_sum, n = 0, 0\n",
    "    net.eval()  # inference mode\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            acc_sum += accuracy(net(X), y)\n",
    "            n += y.shape[0]\n",
    "    net.train()  # back to training mode\n",
    "    return acc_sum / n\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Training loop\n",
    "# ------------------------------\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss_fn(y_hat, y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss_sum += l.item() * y.shape[0]\n",
    "        train_acc_sum += accuracy(y_hat, y)\n",
    "        n += y.shape[0]\n",
    "    \n",
    "    test_acc = evaluate_accuracy(net, test_iter)\n",
    "    print(f'Epoch {epoch+1:2d} | '\n",
    "          f'Loss {train_loss_sum/n:.4f} | '\n",
    "          f'Train Acc {train_acc_sum/n:.4f} | '\n",
    "          f'Test Acc {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87f3bde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: Trouser, Predicted: Trouser\n",
      "Class probabilities:\n",
      "tensor([3.2408e-04, 9.8510e-01, 2.1271e-04, 1.3038e-02, 1.3090e-03, 6.0937e-10,\n",
      "        9.5145e-06, 7.5797e-07, 3.1912e-06, 5.2056e-07])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick a random image from the test set\n",
    "X, y = next(iter(test_iter))  # a batch\n",
    "i = torch.randint(0, len(X), (1,)).item()\n",
    "\n",
    "img = X[i]\n",
    "true_label = y[i].item()\n",
    "\n",
    "# Get prediction (must add batch dimension)\n",
    "with torch.no_grad():\n",
    "    logits = net(img.unsqueeze(0))          # shape: [1, 10]\n",
    "    probs = torch.softmax(logits, dim=1)    # convert to probabilities\n",
    "    pred_label = probs.argmax(dim=1).item() # highest probability index\n",
    "classes = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "print(f\"True label: {classes[true_label]}, Predicted: {classes[pred_label]}\")\n",
    "print(f\"Class probabilities:\\n{probs.squeeze()}\")\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"True label: {classes[true_label]}, Predicted: {classes[pred_label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env_temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
