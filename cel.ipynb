{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0535b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))  \n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5d5f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.float32 torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=32)\n",
    "for X, y in train_iter:\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0752461e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "net.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d8aa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,accuracy 30.000000\n",
      "epoch 2,accuracy 25.000000\n",
      "epoch 3,accuracy 30.000000\n",
      "epoch 4,accuracy 27.000000\n",
      "epoch 5,accuracy 28.000000\n",
      "epoch 6,accuracy 29.000000\n",
      "epoch 7,accuracy 28.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m num_epochs = \u001b[32m150\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = _Image_fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYP_Battery/python_env_temp/lib/python3.13/site-packages/torchvision/transforms/functional.py:174\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    172\u001b[39m img = img.view(pic.size[\u001b[32m1\u001b[39m], pic.size[\u001b[32m0\u001b[39m], F_pil.get_image_num_channels(pic))\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img.to(dtype=default_float_dtype).div(\u001b[32m255\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_iter:\n",
    "        l = loss_fn(net(X), y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    acc = accuracy(net(X), y)\n",
    "    print(f'epoch {epoch + 1},accuracy {acc:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba423466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Loss 0.7325 | Train Acc 0.7241 | Test Acc 0.8303\n",
      "Epoch  2 | Loss 0.4428 | Train Acc 0.8387 | Test Acc 0.8436\n",
      "Epoch  3 | Loss 0.4017 | Train Acc 0.8544 | Test Acc 0.8563\n",
      "Epoch  4 | Loss 0.3793 | Train Acc 0.8626 | Test Acc 0.8524\n",
      "Epoch  5 | Loss 0.3613 | Train Acc 0.8683 | Test Acc 0.8351\n",
      "Epoch  6 | Loss 0.3475 | Train Acc 0.8729 | Test Acc 0.8553\n",
      "Epoch  7 | Loss 0.3372 | Train Acc 0.8780 | Test Acc 0.8636\n",
      "Epoch  8 | Loss 0.3291 | Train Acc 0.8802 | Test Acc 0.8714\n",
      "Epoch  9 | Loss 0.3192 | Train Acc 0.8839 | Test Acc 0.8725\n",
      "Epoch 10 | Loss 0.3103 | Train Acc 0.8869 | Test Acc 0.8538\n",
      "Epoch 11 | Loss 0.3074 | Train Acc 0.8882 | Test Acc 0.8651\n",
      "Epoch 12 | Loss 0.2967 | Train Acc 0.8920 | Test Acc 0.8707\n",
      "Epoch 13 | Loss 0.2914 | Train Acc 0.8921 | Test Acc 0.8707\n",
      "Epoch 14 | Loss 0.2864 | Train Acc 0.8940 | Test Acc 0.8647\n",
      "Epoch 15 | Loss 0.2837 | Train Acc 0.8961 | Test Acc 0.8678\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ Load dataset\n",
    "# ------------------------------\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    \n",
    "    mnist_train = datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)\n",
    "    \n",
    "    return (data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True),\n",
    "            data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False))\n",
    "\n",
    "batch_size = 60\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# ------------------------------\n",
    "# 2️⃣ Define linear model (no ReLU)\n",
    "# ------------------------------\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),         # flatten 28x28 → 784\n",
    "    nn.Linear(784, 100),  # hidden layer\n",
    "    nn.ReLU(),             # activation\n",
    "    nn.Linear(100, 30),    # hidden layer\n",
    "    nn.ReLU(),             # activation\n",
    "    nn.Linear(30, 10)      # output layer\n",
    ")\n",
    "\n",
    "# Initialize weights\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ Define loss and optimizer\n",
    "# ------------------------------\n",
    "loss_fn = nn.CrossEntropyLoss()         # includes Softmax internally\n",
    "\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.85)\n",
    "# ------------------------------\n",
    "# 4️⃣ Accuracy function\n",
    "# ------------------------------\n",
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    return (y_hat == y).float().sum().item()\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    acc_sum, n = 0, 0\n",
    "    net.eval()  # inference mode\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            acc_sum += accuracy(net(X), y)\n",
    "            n += y.shape[0]\n",
    "    net.train()  # back to training mode\n",
    "    return acc_sum / n\n",
    "\n",
    "# ------------------------------\n",
    "# 5️⃣ Training loop\n",
    "# ------------------------------\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss_fn(y_hat, y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss_sum += l.item() * y.shape[0]\n",
    "        train_acc_sum += accuracy(y_hat, y)\n",
    "        n += y.shape[0]\n",
    "    \n",
    "    test_acc = evaluate_accuracy(net, test_iter)\n",
    "    print(f'Epoch {epoch+1:2d} | '\n",
    "          f'Loss {train_loss_sum/n:.4f} | '\n",
    "          f'Train Acc {train_acc_sum/n:.4f} | '\n",
    "          f'Test Acc {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f3bde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: Ankle boot, Predicted: Sandal\n",
      "Class probabilities:\n",
      "tensor([2.2414e-10, 7.8009e-10, 2.9964e-10, 4.6615e-10, 1.9680e-10, 9.9998e-01,\n",
      "        1.0958e-10, 1.7392e-05, 6.1065e-07, 6.3204e-07])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick a random image from the test set\n",
    "X, y = next(iter(test_iter))  # a batch\n",
    "i = torch.randint(0, len(X), (1,)).item()\n",
    "\n",
    "img = X[i]\n",
    "true_label = y[i].item()\n",
    "\n",
    "# Get prediction (must add batch dimension)\n",
    "with torch.no_grad():\n",
    "    logits = net(img.unsqueeze(0))          # shape: [1, 10]\n",
    "    probs = torch.softmax(logits, dim=1)    # convert to probabilities\n",
    "    pred_label = probs.argmax(dim=1).item() # highest probability index\n",
    "classes = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "print(f\"True label: {classes[true_label]}, Predicted: {classes[pred_label]}\")\n",
    "print(f\"Class probabilities:\\n{probs.squeeze()}\")\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"True label: {classes[true_label]}, Predicted: {classes[pred_label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env_temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
